# AI_voyager
이 저장소는 AI(ML&DL)에 대해 탐구하고, 학습한 내용을 기록하기 위해 만들어졌습니다.  
스터디 참가자는 [With-Coding-Cat](https://github.com/With-Coding-Cat), [biscayan](https://github.com/biscayan), [ehsqjfwk99999](https://github.com/ehsqjfwk99999), [jyshin0926](https://github.com/jyshin0926)입니다.  
스터디 내용은 다음과 같습니다.  
## 대회
- [눈바디 AI Challenge](https://aiheroes2021oct.oopy.io/)
    - 기간 : 2021.10.19 ~ 2021.11.21
    - 분야 : computer vision
    - 세부 분야 : semantic segmentation
- [2021 Ego-Vision 손동작 인식 AI 경진대회](https://dacon.io/competitions/official/235805/overview/description)
    - 기간 : 2021.11.22 ~ 2021.11.27
    - 분야 : computer vision
    - 세부 분야 : image classification
- [생육 기간 예측 경진대회](https://dacon.io/competitions/official/235851/overview/description)
    - 기간 : 2021.11.28 ~ 2021.12.17
    - 분야 : computer vision
    - 세부 분야 : prediction
- [뉴스 토픽 분류 AI 경진대회](https://dacon.io/competitions/official/235747/overview/description)
    - 기간 : 2021.12.18 ~ 2022.01.11
    - 분야 : NLP
    - 세부 분야 : classification
- [농업 환경 변화에 따른 작물 병해 진단 AI 경진대회](https://dacon.io/competitions/official/235870/overview/description)
    - 기간 : 2022.01.12 ~ 
    - 분야 : computer vision
    - 세부 분야 : 

## 논문
- [Deep Residual Learning for Image Recognition](https://cs.colby.edu/courses/S16/cs365/papers/he-deepLearningOR-CVPR15.pdf)
- [EfficientNet: Rethinking model scaling for convolutional neural network](https://arxiv.org/pdf/1905.11946.pdf)
- [Rich feature hierarchies for accurate object detection and semantic segmentation](https://arxiv.org/abs/1311.2524)
- [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://proceedings.neurips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf)
- [You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/abs/1506.02640)
- [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
- [Language Models are Unsupervised Multitask Learners](http://www.persagen.com/files/misc/radford2019language.pdf)
- [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)
- [Big Bird: Transformers for Longer Sequences](https://arxiv.org/pdf/2007.14062.pdf)